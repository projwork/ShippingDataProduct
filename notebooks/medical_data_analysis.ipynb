{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Ethiopian Medical Telegram Data Analysis\n",
        "\n",
        "This notebook provides comprehensive analysis of scraped data from Ethiopian medical and healthcare Telegram channels.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This analysis covers:\n",
        "- **Data Exploration**: Understanding the structure and content of scraped messages\n",
        "- **Text Analysis**: Extracting insights from message content\n",
        "- **Media Analysis**: Analyzing downloaded images and media files\n",
        "- **Channel Comparison**: Comparing activity across different medical channels\n",
        "- **Temporal Analysis**: Understanding posting patterns over time\n",
        "- **Content Categorization**: Classifying messages by medical categories\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "1. Successfully scraped data using `scripts/medical_telegram_scraper.py`\n",
        "2. Data available in `data/raw/telegram_messages/`\n",
        "3. Images available in `data/images/`\n",
        "4. All required packages installed (`pip install -r requirements.txt`)\n",
        "\n",
        "## Data Structure\n",
        "\n",
        "The scraped data follows this structure:\n",
        "```\n",
        "data/\n",
        "â”œâ”€â”€ raw/\n",
        "â”‚   â””â”€â”€ telegram_messages/\n",
        "â”‚       â””â”€â”€ YYYY-MM-DD/\n",
        "â”‚           â”œâ”€â”€ individual_channel_files.json\n",
        "â”‚           â”œâ”€â”€ combined_medical_channels.json\n",
        "â”‚           â””â”€â”€ scrape_metadata.json\n",
        "â”œâ”€â”€ images/\n",
        "â”‚   â””â”€â”€ [channel_name]/\n",
        "â”‚       â””â”€â”€ [downloaded_images]\n",
        "â””â”€â”€ logs/\n",
        "    â””â”€â”€ telegram_scraper_YYYYMMDD.log\n",
        "```\n",
        "\n",
        "Let's start the analysis!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Interactive visualizations\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style for matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"ðŸ“Š Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and paths\n",
        "DATA_DIR = Path(\"../data\")\n",
        "RAW_DATA_DIR = DATA_DIR / \"raw\" / \"telegram_messages\"\n",
        "IMAGES_DIR = DATA_DIR / \"images\"\n",
        "LOGS_DIR = DATA_DIR / \"logs\"\n",
        "\n",
        "# Find the most recent data directory\n",
        "available_dates = [d for d in RAW_DATA_DIR.glob(\"*\") if d.is_dir()]\n",
        "if not available_dates:\n",
        "    raise FileNotFoundError(\"No scraped data found! Please run the scraper first.\")\n",
        "\n",
        "latest_date = max(available_dates, key=lambda x: x.name)\n",
        "print(f\"ðŸ“… Using data from: {latest_date.name}\")\n",
        "\n",
        "# Data file paths\n",
        "COMBINED_DATA_FILE = latest_date / \"combined_medical_channels.json\"\n",
        "METADATA_FILE = latest_date / \"scrape_metadata.json\"\n",
        "\n",
        "# Check if files exist\n",
        "if not COMBINED_DATA_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Combined data file not found: {COMBINED_DATA_FILE}\")\n",
        "if not METADATA_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Metadata file not found: {METADATA_FILE}\")\n",
        "\n",
        "print(f\"ðŸ“ Data directory: {latest_date}\")\n",
        "print(f\"ðŸ“Š Combined data file: {COMBINED_DATA_FILE.name}\")\n",
        "print(f\"ðŸ“‹ Metadata file: {METADATA_FILE.name}\")\n",
        "print(f\"ðŸ–¼ï¸  Images directory: {IMAGES_DIR}\")\n",
        "print(\"âœ… Configuration completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and examine the data\n",
        "print(\"ðŸ” Loading scraped data...\")\n",
        "\n",
        "# Load combined data\n",
        "with open(COMBINED_DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Load metadata\n",
        "with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(raw_data)\n",
        "print(f\"ðŸ“Š Loaded {len(df)} messages\")\n",
        "\n",
        "# Display metadata\n",
        "print(\"\\nðŸ“‹ Scraping Metadata:\")\n",
        "print(f\"  Scrape Date: {metadata.get('scrape_date', 'Unknown')}\")\n",
        "print(f\"  Total Messages: {metadata.get('total_messages', 'Unknown'):,}\")\n",
        "print(f\"  Channels Scraped: {len(metadata.get('channels_scraped', []))}\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ Channel Statistics:\")\n",
        "for channel, count in metadata.get('channel_counts', {}).items():\n",
        "    print(f\"  {channel}: {count:,} messages\")\n",
        "\n",
        "print(\"\\nðŸ“Š DataFrame Info:\")\n",
        "print(f\"  Shape: {df.shape}\")\n",
        "print(f\"  Columns: {list(df.columns)}\")\n",
        "print(f\"  Memory Usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nðŸ“‹ Sample Data:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and cleaning\n",
        "print(\"ðŸ§¹ Preprocessing data...\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Add derived columns\n",
        "df['hour'] = df['date'].dt.hour\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "df['month'] = df['date'].dt.month\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "# Clean text data\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['has_media'] = df['media_type'].notna()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "\n",
        "# Clean channel names for better display\n",
        "df['channel_clean'] = df['channel'].str.replace('@', '', regex=False)\n",
        "\n",
        "# Filter out empty messages\n",
        "df_clean = df[df['text'].str.len() > 0].copy()\n",
        "\n",
        "print(f\"ðŸ“Š Data after preprocessing:\")\n",
        "print(f\"  Total messages: {len(df):,}\")\n",
        "print(f\"  Messages with text: {len(df_clean):,}\")\n",
        "print(f\"  Messages with media: {df['has_media'].sum():,}\")\n",
        "print(f\"  Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nðŸ“ˆ Basic Statistics:\")\n",
        "print(df_clean[['text_length', 'word_count', 'views', 'forwards']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Channel Analysis and Comparison\n",
        "print(\"ðŸ“Š Analyzing channel activity...\")\n",
        "\n",
        "# Channel statistics\n",
        "channel_stats = df_clean.groupby('channel_clean').agg({\n",
        "    'id': 'count',\n",
        "    'text_length': ['mean', 'median'],\n",
        "    'word_count': ['mean', 'median'],\n",
        "    'views': ['mean', 'median', 'max'],\n",
        "    'forwards': ['mean', 'median', 'max'],\n",
        "    'has_media': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "channel_stats.columns = ['Messages', 'Avg_Text_Length', 'Median_Text_Length', \n",
        "                        'Avg_Word_Count', 'Median_Word_Count', 'Avg_Views', \n",
        "                        'Median_Views', 'Max_Views', 'Avg_Forwards', \n",
        "                        'Median_Forwards', 'Max_Forwards', 'Media_Count']\n",
        "\n",
        "print(\"ðŸ“ˆ Channel Statistics:\")\n",
        "print(channel_stats.sort_values('Messages', ascending=False))\n",
        "\n",
        "# Visualization: Channel message counts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Messages per channel\n",
        "channel_counts = df_clean['channel_clean'].value_counts()\n",
        "axes[0, 0].bar(channel_counts.index, channel_counts.values, color='skyblue')\n",
        "axes[0, 0].set_title('Messages per Channel')\n",
        "axes[0, 0].set_xlabel('Channel')\n",
        "axes[0, 0].set_ylabel('Message Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Average views per channel\n",
        "avg_views = df_clean.groupby('channel_clean')['views'].mean()\n",
        "axes[0, 1].bar(avg_views.index, avg_views.values, color='lightgreen')\n",
        "axes[0, 1].set_title('Average Views per Channel')\n",
        "axes[0, 1].set_xlabel('Channel')\n",
        "axes[0, 1].set_ylabel('Average Views')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Media distribution\n",
        "media_counts = df_clean.groupby('channel_clean')['has_media'].sum()\n",
        "axes[1, 0].bar(media_counts.index, media_counts.values, color='orange')\n",
        "axes[1, 0].set_title('Media Messages per Channel')\n",
        "axes[1, 0].set_xlabel('Channel')\n",
        "axes[1, 0].set_ylabel('Media Count')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Text length distribution\n",
        "df_clean.boxplot(column='text_length', by='channel_clean', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Text Length Distribution by Channel')\n",
        "axes[1, 1].set_xlabel('Channel')\n",
        "axes[1, 1].set_ylabel('Text Length')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal Analysis\n",
        "print(\"â° Analyzing temporal patterns...\")\n",
        "\n",
        "# Messages over time\n",
        "daily_messages = df_clean.groupby(df_clean['date'].dt.date).size()\n",
        "hourly_messages = df_clean.groupby('hour').size()\n",
        "dow_messages = df_clean.groupby('day_of_week').size()\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Daily message trend\n",
        "axes[0, 0].plot(daily_messages.index, daily_messages.values, marker='o')\n",
        "axes[0, 0].set_title('Daily Message Trend')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Message Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Hourly distribution\n",
        "axes[0, 1].bar(hourly_messages.index, hourly_messages.values, color='lightblue')\n",
        "axes[0, 1].set_title('Messages by Hour of Day')\n",
        "axes[0, 1].set_xlabel('Hour')\n",
        "axes[0, 1].set_ylabel('Message Count')\n",
        "\n",
        "# Day of week distribution\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "dow_ordered = dow_messages.reindex(day_order)\n",
        "axes[1, 0].bar(dow_ordered.index, dow_ordered.values, color='lightgreen')\n",
        "axes[1, 0].set_title('Messages by Day of Week')\n",
        "axes[1, 0].set_xlabel('Day of Week')\n",
        "axes[1, 0].set_ylabel('Message Count')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Heatmap: Hour vs Day of Week\n",
        "pivot_data = df_clean.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)\n",
        "pivot_data = pivot_data.reindex(day_order)\n",
        "im = axes[1, 1].imshow(pivot_data, cmap='YlOrRd', aspect='auto')\n",
        "axes[1, 1].set_title('Message Heatmap: Hour vs Day of Week')\n",
        "axes[1, 1].set_xlabel('Hour')\n",
        "axes[1, 1].set_ylabel('Day of Week')\n",
        "axes[1, 1].set_yticks(range(len(day_order)))\n",
        "axes[1, 1].set_yticklabels(day_order)\n",
        "plt.colorbar(im, ax=axes[1, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Peak activity analysis\n",
        "print(\"\\nðŸ”¥ Peak Activity Analysis:\")\n",
        "peak_hour = hourly_messages.idxmax()\n",
        "peak_day = dow_messages.idxmax()\n",
        "most_active_date = daily_messages.idxmax()\n",
        "\n",
        "print(f\"  Peak Hour: {peak_hour}:00 ({hourly_messages[peak_hour]} messages)\")\n",
        "print(f\"  Peak Day: {peak_day} ({dow_messages[peak_day]} messages)\")\n",
        "print(f\"  Most Active Date: {most_active_date} ({daily_messages[most_active_date]} messages)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text Analysis and Medical Content Classification\n",
        "print(\"ðŸ“ Analyzing text content...\")\n",
        "\n",
        "# Combine all text for analysis\n",
        "all_text = ' '.join(df_clean['text'].astype(str))\n",
        "\n",
        "# Medical keywords for classification\n",
        "medical_keywords = {\n",
        "    'pharmacy': ['áˆ˜á‹µáŠƒáŠ’á‰µ', 'áŠªáŠ’áŠ•', 'á‰…áˆáŒ¡', 'pharmacy', 'medicine', 'drug', 'tablet', 'capsule', 'syrup'],\n",
        "    'cosmetics': ['á‰†á‹³', 'á‹ˆáˆˆáˆ', 'áŒáˆ®áˆ', 'cosmetic', 'beauty', 'skin', 'face', 'cream', 'lotion'],\n",
        "    'medical_equipment': ['áˆ˜áˆ³áˆªá‹«', 'áˆ›áˆ½áŠ•', 'equipment', 'device', 'machine', 'tool', 'instrument'],\n",
        "    'healthcare': ['áŒ¤áŠ“', 'áˆ•áŠ­áˆáŠ“', 'á‹¶áŠ­á‰°áˆ­', 'health', 'doctor', 'clinic', 'hospital', 'treatment'],\n",
        "    'wellness': ['áˆá‹áˆµ', 'á‰ªá‰³áˆšáŠ•', 'wellness', 'vitamin', 'supplement', 'nutrition', 'fitness']\n",
        "}\n",
        "\n",
        "# Categorize messages\n",
        "def categorize_message(text):\n",
        "    text_lower = text.lower()\n",
        "    categories = []\n",
        "    \n",
        "    for category, keywords in medical_keywords.items():\n",
        "        if any(keyword in text_lower for keyword in keywords):\n",
        "            categories.append(category)\n",
        "    \n",
        "    return categories if categories else ['other']\n",
        "\n",
        "# Apply categorization\n",
        "df_clean['categories'] = df_clean['text'].apply(categorize_message)\n",
        "df_clean['primary_category'] = df_clean['categories'].apply(lambda x: x[0] if x else 'other')\n",
        "\n",
        "# Category distribution\n",
        "category_counts = df_clean['primary_category'].value_counts()\n",
        "print(\"ðŸ“Š Message Categories:\")\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"  {category}: {count:,} messages ({count/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "# Visualize categories\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Category distribution pie chart\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Message Categories Distribution')\n",
        "\n",
        "# Category by channel\n",
        "plt.subplot(2, 2, 2)\n",
        "category_channel = pd.crosstab(df_clean['channel_clean'], df_clean['primary_category'])\n",
        "category_channel.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "plt.title('Categories by Channel')\n",
        "plt.xlabel('Channel')\n",
        "plt.ylabel('Message Count')\n",
        "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Word cloud\n",
        "plt.subplot(2, 1, 2)\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of All Messages')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Media and Image Analysis\n",
        "print(\"ðŸ–¼ï¸ Analyzing media content...\")\n",
        "\n",
        "# Media statistics\n",
        "media_stats = df_clean[df_clean['has_media']].groupby('channel_clean').agg({\n",
        "    'media_type': 'count',\n",
        "    'media_size': ['mean', 'sum']\n",
        "}).round(2)\n",
        "\n",
        "if not media_stats.empty:\n",
        "    media_stats.columns = ['Media_Count', 'Avg_Size_MB', 'Total_Size_MB']\n",
        "    print(\"ðŸ“Š Media Statistics by Channel:\")\n",
        "    print(media_stats)\n",
        "\n",
        "# Analyze media types\n",
        "media_types = df_clean['media_type'].value_counts()\n",
        "print(f\"\\nðŸ“ˆ Media Types Distribution:\")\n",
        "for media_type, count in media_types.items():\n",
        "    print(f\"  {media_type}: {count:,} files\")\n",
        "\n",
        "# Visualize media analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Media type distribution\n",
        "if not media_types.empty:\n",
        "    axes[0, 0].pie(media_types.values, labels=media_types.index, autopct='%1.1f%%')\n",
        "    axes[0, 0].set_title('Media Types Distribution')\n",
        "\n",
        "# Media count by channel\n",
        "media_by_channel = df_clean.groupby('channel_clean')['has_media'].sum()\n",
        "axes[0, 1].bar(media_by_channel.index, media_by_channel.values, color='orange')\n",
        "axes[0, 1].set_title('Media Count by Channel')\n",
        "axes[0, 1].set_xlabel('Channel')\n",
        "axes[0, 1].set_ylabel('Media Count')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Media size distribution\n",
        "media_with_size = df_clean[df_clean['media_size'] > 0]\n",
        "if not media_with_size.empty:\n",
        "    axes[1, 0].hist(media_with_size['media_size'], bins=30, alpha=0.7, color='lightblue')\n",
        "    axes[1, 0].set_title('Media Size Distribution')\n",
        "    axes[1, 0].set_xlabel('Size (bytes)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Media over time\n",
        "media_over_time = df_clean[df_clean['has_media']].groupby(df_clean['date'].dt.date).size()\n",
        "if not media_over_time.empty:\n",
        "    axes[1, 1].plot(media_over_time.index, media_over_time.values, marker='o', color='red')\n",
        "    axes[1, 1].set_title('Media Messages Over Time')\n",
        "    axes[1, 1].set_xlabel('Date')\n",
        "    axes[1, 1].set_ylabel('Media Count')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Image analysis (if images exist)\n",
        "if IMAGES_DIR.exists():\n",
        "    print(\"\\nðŸ–¼ï¸ Image Analysis:\")\n",
        "    image_channels = [d for d in IMAGES_DIR.iterdir() if d.is_dir()]\n",
        "    \n",
        "    for channel_dir in image_channels:\n",
        "        image_files = list(channel_dir.glob(\"*.jpg\")) + list(channel_dir.glob(\"*.png\"))\n",
        "        print(f\"  {channel_dir.name}: {len(image_files)} images\")\n",
        "        \n",
        "        # Analyze a sample of images\n",
        "        if image_files:\n",
        "            sample_images = image_files[:5]  # Analyze first 5 images\n",
        "            \n",
        "            for img_path in sample_images:\n",
        "                try:\n",
        "                    img = Image.open(img_path)\n",
        "                    print(f\"    {img_path.name}: {img.size} pixels, {img.mode} mode\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    {img_path.name}: Error reading - {e}\")\n",
        "else:\n",
        "    print(\"ðŸ“ No images directory found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Analytics and Insights\n",
        "print(\"ðŸ” Generating advanced insights...\")\n",
        "\n",
        "# Engagement analysis\n",
        "df_clean['engagement_score'] = df_clean['views'] + (df_clean['forwards'] * 2)\n",
        "df_clean['engagement_per_word'] = df_clean['engagement_score'] / df_clean['word_count'].replace(0, 1)\n",
        "\n",
        "# Top performing content\n",
        "top_posts = df_clean.nlargest(10, 'engagement_score')[['channel_clean', 'text', 'views', 'forwards', 'engagement_score']]\n",
        "print(\"ðŸ† Top 10 Most Engaging Posts:\")\n",
        "for idx, post in top_posts.iterrows():\n",
        "    print(f\"\\n{post['channel_clean']} (Score: {post['engagement_score']:.0f})\")\n",
        "    print(f\"Views: {post['views']}, Forwards: {post['forwards']}\")\n",
        "    print(f\"Text: {post['text'][:100]}...\")\n",
        "\n",
        "# Sentiment analysis (basic)\n",
        "def simple_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "df_clean['sentiment'] = df_clean['text'].apply(simple_sentiment)\n",
        "df_clean['sentiment_label'] = df_clean['sentiment'].apply(\n",
        "    lambda x: 'positive' if x > 0.1 else 'negative' if x < -0.1 else 'neutral'\n",
        ")\n",
        "\n",
        "# Sentiment distribution\n",
        "sentiment_dist = df_clean['sentiment_label'].value_counts()\n",
        "print(f\"\\nðŸ˜Š Sentiment Distribution:\")\n",
        "for sentiment, count in sentiment_dist.items():\n",
        "    print(f\"  {sentiment}: {count:,} messages ({count/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "# Visualize advanced analytics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Engagement by channel\n",
        "engagement_by_channel = df_clean.groupby('channel_clean')['engagement_score'].mean()\n",
        "axes[0, 0].bar(engagement_by_channel.index, engagement_by_channel.values, color='purple')\n",
        "axes[0, 0].set_title('Average Engagement by Channel')\n",
        "axes[0, 0].set_xlabel('Channel')\n",
        "axes[0, 0].set_ylabel('Engagement Score')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Sentiment distribution\n",
        "axes[0, 1].pie(sentiment_dist.values, labels=sentiment_dist.index, autopct='%1.1f%%')\n",
        "axes[0, 1].set_title('Sentiment Distribution')\n",
        "\n",
        "# Engagement vs Text Length\n",
        "axes[1, 0].scatter(df_clean['text_length'], df_clean['engagement_score'], alpha=0.6)\n",
        "axes[1, 0].set_title('Engagement vs Text Length')\n",
        "axes[1, 0].set_xlabel('Text Length')\n",
        "axes[1, 0].set_ylabel('Engagement Score')\n",
        "\n",
        "# Sentiment by channel\n",
        "sentiment_channel = pd.crosstab(df_clean['channel_clean'], df_clean['sentiment_label'])\n",
        "sentiment_channel.plot(kind='bar', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Sentiment by Channel')\n",
        "axes[1, 1].set_xlabel('Channel')\n",
        "axes[1, 1].set_ylabel('Message Count')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and Recommendations\n",
        "print(\"ðŸ“‹ Analysis Summary and Recommendations\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Key findings\n",
        "print(\"\\nðŸ” KEY FINDINGS:\")\n",
        "print(f\"1. Data Overview:\")\n",
        "print(f\"   - Total messages analyzed: {len(df_clean):,}\")\n",
        "print(f\"   - Active channels: {df_clean['channel_clean'].nunique()}\")\n",
        "print(f\"   - Date range: {df_clean['date'].min().strftime('%Y-%m-%d')} to {df_clean['date'].max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "print(f\"\\n2. Channel Performance:\")\n",
        "most_active_channel = df_clean['channel_clean'].value_counts().index[0]\n",
        "highest_engagement_channel = df_clean.groupby('channel_clean')['engagement_score'].mean().idxmax()\n",
        "print(f\"   - Most active channel: {most_active_channel}\")\n",
        "print(f\"   - Highest engagement: {highest_engagement_channel}\")\n",
        "\n",
        "print(f\"\\n3. Content Analysis:\")\n",
        "top_category = df_clean['primary_category'].value_counts().index[0]\n",
        "avg_text_length = df_clean['text_length'].mean()\n",
        "print(f\"   - Top content category: {top_category}\")\n",
        "print(f\"   - Average message length: {avg_text_length:.0f} characters\")\n",
        "\n",
        "print(f\"\\n4. Temporal Patterns:\")\n",
        "print(f\"   - Peak posting hour: {hourly_messages.idxmax()}:00\")\n",
        "print(f\"   - Most active day: {dow_messages.idxmax()}\")\n",
        "\n",
        "print(f\"\\n5. Media Content:\")\n",
        "media_percentage = (df_clean['has_media'].sum() / len(df_clean)) * 100\n",
        "print(f\"   - Messages with media: {media_percentage:.1f}%\")\n",
        "\n",
        "print(f\"\\n6. Sentiment Analysis:\")\n",
        "positive_pct = (df_clean['sentiment_label'] == 'positive').mean() * 100\n",
        "print(f\"   - Positive sentiment: {positive_pct:.1f}%\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "print(\"1. Content Strategy:\")\n",
        "print(\"   - Focus on high-engagement content types\")\n",
        "print(\"   - Optimize posting times (peak hours)\")\n",
        "print(\"   - Increase media content for better engagement\")\n",
        "\n",
        "print(\"\\n2. Channel Management:\")\n",
        "print(\"   - Learn from top-performing channels\")\n",
        "print(\"   - Analyze successful content patterns\")\n",
        "print(\"   - Monitor sentiment trends\")\n",
        "\n",
        "print(\"\\n3. Data Collection:\")\n",
        "print(\"   - Consider expanding to more channels\")\n",
        "print(\"   - Implement real-time monitoring\")\n",
        "print(\"   - Add more sophisticated NLP analysis\")\n",
        "\n",
        "print(\"\\n4. Business Intelligence:\")\n",
        "print(\"   - Track engagement metrics over time\")\n",
        "print(\"   - Monitor competitor activity\")\n",
        "print(\"   - Identify trending medical topics\")\n",
        "\n",
        "print(\"\\nðŸ“Š NEXT STEPS:\")\n",
        "print(\"1. Set up automated daily scraping\")\n",
        "print(\"2. Implement real-time dashboard\")\n",
        "print(\"3. Build predictive models for engagement\")\n",
        "print(\"4. Create content recommendation system\")\n",
        "print(\"5. Develop image classification for medical products\")\n",
        "\n",
        "print(f\"\\nâœ… Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"ðŸ“ All results saved to the data directory\")\n",
        "print(\"ðŸŽ¯ Ready for next phase of analysis!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
